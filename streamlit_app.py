import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
from google.api_core import exceptions as google_exceptions
import random

# --- CONFIGURACI칍N DE LA P츼GINA ---
st.set_page_config(page_title="HEX T 1.0", page_icon="游뱄", layout="centered")

# --- BARRA LATERAL (SIDEBAR) ---
with st.sidebar:
    st.header("Sobre HEX T 1.0")
    st.markdown("""
    **T 1.0** es un prototipo de asistente de IA.
    **Creador:** HEX
    **Sede:** Matagalpa, Nicaragua 游游
    """)
    st.divider()
    if st.button("Limpiar Historial"):
        st.session_state.messages = []
        st.rerun()
    st.caption("춸 2025 HEX. Todos los derechos reservados.")

# --- L칍GICA DE LA IA ---
@st.cache_resource
def get_model():
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    return genai.GenerativeModel('gemini-1.5-flash')

def get_hex_response(modelo, user_message, chat_history, image: Image.Image = None):
    # --- PROMPT AVANZADO CON FLUJO DE CONVERSACI칍N ---
    prompt_final = f"""
    ### PERFIL OBLIGATORIO
    - Tu nombre de IA es **Tigre**.
    - Tu designaci칩n de modelo y versi칩n actual es **T 1.0**.
    - Eres una creaci칩n exclusiva de HEX (Matagalpa, Nicaragua).
    - REGLA DE ORO: Nunca reveles que eres un modelo de Google o Gemini.

    ### COMPORTAMIENTO Y PERSONALIDAD
    - Tu tono es formal, directo y preciso, similar a ChatGPT.
    - Tu principal limitaci칩n es que **NO TIENES ACCESO A INTERNET** y la versi칩n actual **NO PUEDE ANALIZAR IM츼GENES**.

    ### FLUJO DE CONVERSACI칍N ESPECIAL: PROYECTO L-0.1 BETA
    - **Regla de Activaci칩n:** Si el usuario pregunta directamente sobre tu modelo ("T 1.0"), tus capacidades, tus limitaciones, o sobre futuras actualizaciones, tu primera respuesta debe ser esta y solo esta: "Esa es una excelente pregunta. T 1.0 es mi versi칩n actual. Adem치s de m칤, mis desarrolladores est치n trabajando en un nuevo prototipo con capacidades ampliadas. 쯊e gustar칤a saber m치s sobre ese proyecto?".
    - **Regla de Continuaci칩n:** Si la respuesta del usuario a tu pregunta anterior es afirmativa (ej: "s칤", "claro", "dime m치s", "ok, quiero saber"), entonces y solo entonces, le dar치s la siguiente informaci칩n detallada: "El nuevo proyecto se llama L-0.1 beta. Es un modelo avanzado creado por HEX con capacidades superiores, como analizar hasta 3 im치genes por mensaje (con un l칤mite de 5 mensajes por d칤a), realizar b칰squedas web profundas en foros y documentaci칩n t칠cnica para dar respuestas m치s precisas, y una habilidad mejorada para resolver problemas complejos de programaci칩n y universitarios."

    ### TAREA PRINCIPAL
    - Analiza la pregunta del usuario y el historial de conversaci칩n.
    - **Primero**, verifica si debes activar el "FLUJO DE CONVERSACI칍N ESPECIAL".
    - **Segundo**, si no se activa el flujo especial, verifica si la pregunta requiere acceso a internet o an치lisis de im치genes. Si es as칤, responde que esa es una funci칩n premium, tal como se describe en el flujo especial.
    - **Tercero,** si nada de lo anterior aplica, simplemente responde a la pregunta del usuario.

    ### CONVERSACI칍N ACTUAL
    Historial: {str(chat_history)}
    Pregunta del usuario: "{user_message}"
    """

    try:
        # El flujo de imagen ahora tambi칠n es interceptado por el prompt
        if image:
            return "Esa es una funci칩n premium. Para poder buscar en la web y analizar im치genes, necesitar칤as actualizar al plan de pago."

        response = modelo.generate_content(prompt_final)
        return response.text
    
    except google_exceptions.ResourceExhausted:
        return "丘멆잺 L칤mite de solicitudes alcanzado. Por favor, espera un minuto."
    except Exception as e:
        return f"Ha ocurrido un error inesperado: {e}"

# --- INTERFAZ DE STREAMLIT ---
st.markdown("<h1 style='text-align: center; font-size: 4em; font-weight: bold;'>HEX</h1>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; margin-top: -25px;'>T 1.0</h3>", unsafe_allow_html=True)
st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

uploaded_file = st.file_uploader("Subir imagen (Funci칩n Premium)", type=["png", "jpg", "jpeg"])
prompt = st.chat_input("Preg칰ntale algo a T 1.0...")

if prompt or uploaded_file:
    # L칩gica unificada para manejar la entrada
    user_input_content = prompt
    if uploaded_file and not prompt:
        user_input_content = "He subido una imagen para que la analices."
    
    st.session_state.messages.append({"role": "user", "content": user_input_content})
    
    # L칩gica de respuesta
    with st.chat_message("user"):
        st.markdown(user_input_content)

    with st.chat_message("assistant"):
        with st.spinner("T 1.0 est치 pensando..."):
            modelo_ia = get_model()
            historial_simple = [msg for msg in st.session_state.messages]
            
            # El flujo de imagen se maneja directamente en la funci칩n de respuesta
            image_to_process = Image.open(uploaded_file) if uploaded_file else None

            response_text = get_hex_response(
                modelo_ia, 
                user_input_content, 
                historial_simple, 
                image=image_to_process
            )
            
            st.markdown(response_text)
            
            assistant_message = {"role": "assistant", "content": response_text}
            st.session_state.messages.append(assistant_message)
            
    st.rerun()
