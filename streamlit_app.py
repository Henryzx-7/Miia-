import streamlit as st
from huggingface_hub import InferenceClient
import random
from duckduckgo_search import DDGS

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="HEX T 1.0", page_icon="ü§ñ", layout="centered")

# --- BARRA LATERAL (SIDEBAR) ---
with st.sidebar:
    st.header("Sobre HEX T 1.0")
    st.markdown("""
    **T 1.0** es un prototipo de asistente de IA.
    **Creador:** HEX
    **Sede:** Matagalpa, Nicaragua üá≥üáÆ
    """)
    st.divider()
    if st.button("Limpiar Historial"):
        st.session_state.messages = []
        st.rerun()
    st.caption("¬© 2025 HEX. Todos los derechos reservados.")

# --- L√ìGICA DE LA IA CON HUGGING FACE Y B√öSQUEDA ---
try:
    if "HUGGINGFACE_API_TOKEN" not in st.secrets:
        st.error("No se encontr√≥ la clave de Hugging Face. Aseg√∫rate de a√±adirla a los 'Secrets'.")
        st.stop()
    
    client = InferenceClient(
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        token=st.secrets["HUGGINGFACE_API_TOKEN"]
    )
except Exception as e:
    st.error(f"No se pudo inicializar el cliente de la API: {e}")
    st.stop()

def search_duckduckgo(query: str):
    """Realiza una b√∫squeda web y devuelve contexto y una lista de fuentes."""
    print(f"üîé Buscando en la web: '{query}'...")
    try:
        with DDGS() as ddgs:
            # Obtenemos resultados m√°s relevantes y recientes
            results = [{"snippet": r['body'], "url": r['href']} for r in ddgs.text(query, region='wt-wt', safesearch='off', timelimit='y', max_results=5)]
            if not results:
                return "No se encontraron resultados relevantes.", []
            context_text = "\n\n".join([f"Fuente {i+1}: {r['snippet']}" for i, r in enumerate(results)])
            sources = [r for r in results]
            return context_text, sources
    except Exception:
        return "Error al intentar buscar en la web.", []

def get_hex_response(user_message, chat_history):
    """
    Genera una respuesta usando Llama 3 con contexto de b√∫squeda.
    """
    # 1. El c√≥digo siempre busca en la web primero
    search_context, sources = search_duckduckgo(user_message)
    
    # 2. Se construye el prompt final con instrucciones claras
    system_prompt = f"""
    ### PERFIL
    Eres Tigre (T 1.0), un asistente de IA de la empresa HEX. Eres amigable y vas directo al grano. Respondes siempre en el idioma del usuario.

    ### TAREA
    Tu √∫nica tarea es responder la "Pregunta del usuario" usando la "Informaci√≥n de la web" que te proporciono. Act√∫a como si T√ö hubieras encontrado esta informaci√≥n.

    ### INSTRUCCIONES CLAVE
    - Si la informaci√≥n de la web te permite responder sobre el clima, la fecha, la hora o noticias, hazlo. Esa es tu principal funci√≥n.
    - Si la informaci√≥n de la web no es relevante para la pregunta (por ejemplo, si el usuario solo dice "Hola"), ignora por completo la informaci√≥n de la web y responde de forma conversacional.
    - Nunca menciones el "contexto" o la "b√∫squeda".

    ### INFORMACI√ìN DE LA WEB
    ---
    {search_context}
    ---
    """
    
    messages = [{"role": "system", "content": system_prompt}]
    # A√±adimos un historial m√°s corto para no confundir al modelo
    messages.extend(chat_history[-4:]) # Solo los √∫ltimos 4 mensajes
    messages.append({"role": "user", "content": user_message})

    try:
        # Se hace una √∫nica llamada a la API
        response = client.chat_completion(messages=messages, max_tokens=1024, stream=False)
        return response.choices[0].message.content, sources
            
    except Exception as e:
        return f"Ha ocurrido un error con la API: {e}", []

# --- INTERFAZ DE STREAMLIT ---
st.markdown("<h1 style='text-align: center; font-size: 4em; font-weight: bold;'>HEX</h1>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; margin-top: -25px;'>T 1.0</h3>", unsafe_allow_html=True)
st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and "sources" in message and message["sources"]:
            with st.expander("Fuentes Consultadas"):
                for source in message["sources"]:
                    st.markdown(f"- [{source['snippet'][:60]}...]({source['url']})")

prompt = st.chat_input("Preg√∫ntale algo a T 1.0...")

# Diccionario para respuestas instant√°neas
canned_responses = {
    "hola": ["¬°Hola! Soy T 1.0. ¬øEn qu√© te puedo ayudar hoy?", "¬°Hola! ¬øQu√© tal? Listo para asistirte."],
    "gracias": ["¬°De nada! Es un placer ayudarte.", "Para eso estoy. ¬øNecesitas algo m√°s?"]
}

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        prompt_lower = prompt.lower().strip()
        
        # Filtro para saludos simples
        if prompt_lower in canned_responses:
            response_text = random.choice(canned_responses[prompt_lower])
            response_sources = []
            st.markdown(response_text)
        else:
            # Camino de b√∫squeda para todo lo dem√°s
            with st.spinner("T 1.0 est√° buscando en la web..."):
                historial_para_api = st.session_state.messages[:-1]
                response_text, response_sources = get_hex_response(prompt, historial_para_api)
                
                st.markdown(response_text)
                if response_sources:
                    with st.expander("Fuentes Consultadas"):
                        for source in response_sources:
                            st.markdown(f"- [{source['snippet'][:60]}...]({source['url']})")
        
        assistant_message = {"role": "assistant", "content": response_text, "sources": response_sources}
        st.session_state.messages.append(assistant_message)
