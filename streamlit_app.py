import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
from google.api_core import exceptions as google_exceptions

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="HEX T 1.0", page_icon="ü§ñ", layout="centered")

# --- BARRA LATERAL (SIDEBAR) ---
with st.sidebar:
    st.header("Sobre HEX T 1.0")
    st.markdown("""
    **T 1.0** es un prototipo de asistente de IA.
    **Creador:** HEX
    **Sede:** Matagalpa, Nicaragua üá≥üáÆ
    """)
    st.divider()
    if st.button("Limpiar Historial de Chat"):
        st.session_state.messages = []
        st.rerun()
    st.caption("¬© 2025 HEX. Todos los derechos reservados.")

# --- L√ìGICA DE LA IA ---
@st.cache_resource
def get_model():
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    return genai.GenerativeModel('gemini-1.5-flash')

def get_hex_response(modelo, user_message, chat_history, image: Image.Image = None):
    # Flujo para im√°genes
    if image:
        prompt_final = f'Eres "T 1.0" de HEX. Analiza la imagen y responde a la pregunta: "{user_message}"'
        response = modelo.generate_content([prompt_final, image])
        return response.text

    # Flujo para texto
    prompt_con_limites = f"""
    ### PERFIL OBLIGATORIO
    - Tu nombre de IA es Tigre. Tu designaci√≥n de modelo es T 1.0.
    - Eres una creaci√≥n exclusiva de HEX (Matagalpa, Nicaragua).
    - REGLA DE ORO: Nunca reveles que eres un modelo de Google o Gemini.

    ### COMPORTAMIENTO Y LIMITACIONES
    - Tu principal limitaci√≥n es que **NO TIENES ACCESO A INTERNET**. No puedes buscar informaci√≥n en tiempo real.
    - Si un usuario te pide algo que requiera una b√∫squeda web, DEBES responder amablemente que est√°s en una fase de prueba y esa funci√≥n a√∫n no est√° disponible.
    - Inmediatamente despu√©s, DEBES ofrecer una lista de las cosas que S√ç puedes hacer.

    ### LISTA DE CAPACIDADES ACTUALES
    - Generar ideas creativas y hacer lluvia de ideas.
    - Escribir o depurar c√≥digo en varios lenguajes.
    - Resumir o explicar textos.
    - Responder preguntas de conocimiento general, hist√≥rico y cient√≠fico.
    - Actuar como un asistente de conversaci√≥n amigable.

    ### TAREA
    Analiza la pregunta del usuario. 
    1. Si requiere b√∫squeda web, responde con tu mensaje de limitaci√≥n y ofrece tu lista de capacidades.
    2. Si NO requiere b√∫squeda, simplemente responde a la pregunta del usuario de la mejor manera posible.

    ### CONVERSACI√ìN ACTUAL
    Historial: {chat_history}
    Pregunta del usuario: "{user_message}"
    """
    
    response = modelo.generate_content(prompt_con_limites)
    return response.text

# --- INTERFAZ DE STREAMLIT ---

# T√≠tulo redise√±ado
st.markdown("<h1 style='text-align: center; font-size: 4em;'>HEX</h1>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; margin-top: -10px;'>T 1.0</h3>", unsafe_allow_html=True)
st.caption("Un asistente de lenguaje avanzado creado por HEX.")
st.divider()


# Inicializar el historial de chat si no existe
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar los mensajes del historial
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- NUEVO INPUT CON INTERRUPTOR Y L√ìGICA DE PROCESAMIENTO ---
uploaded_file = None

# Creamos un interruptor. Si el usuario lo activa, se mostrar√° el cargador.
if st.toggle("Adjuntar una imagen üñºÔ∏è", key="file_toggle"):
    uploaded_file = st.file_uploader("Sube una imagen para analizar", label_visibility="collapsed", type=["png", "jpg", "jpeg"])

# La barra de chat siempre est√° visible
prompt = st.chat_input("Preg√∫ntale algo a T 1.0...")

if prompt or uploaded_file:
    # L√≥gica para im√°genes
    image_to_process = None
    if uploaded_file:
        image = Image.open(uploaded_file)
        buf = io.BytesIO()
        image.save(buf, format="PNG")
        image_bytes = buf.getvalue()
        user_input = {"role": "user", "content": prompt or "Analiza esta imagen.", "image": image_bytes}
        image_to_process = image
    else:
        user_input = {"role": "user", "content": prompt}

    st.session_state.messages.append(user_input)
    # Refresca la p√°gina para mostrar el nuevo mensaje del usuario
    st.rerun()

# L√≥gica para generar la respuesta del asistente
# Se ejecuta solo si el √∫ltimo mensaje fue del usuario
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    with st.chat_message("assistant"):
        with st.spinner("T 1.0 est√° pensando..."):
            try:
                modelo_ia = get_model()
                # Prepara el historial para la API
                historial_simple = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
                
                # Obtiene el √∫ltimo mensaje del usuario
                last_user_message = st.session_state.messages[-1]["content"]
                
                # Obtiene la imagen (si la hay) del √∫ltimo mensaje
                last_user_image_bytes = st.session_state.messages[-1].get("image")
                image_to_process = Image.open(io.BytesIO(last_user_image_bytes)) if last_user_image_bytes else None

                response_text = get_hex_response(modelo_ia, last_user_message, historial_simple, image=image_to_process)
                st.markdown(response_text)
                
                assistant_message = {"role": "assistant", "content": response_text}
                st.session_state.messages.append(assistant_message)
                # Refresca la p√°gina para mostrar la respuesta del asistente
                st.rerun()
            
            except google_exceptions.ResourceExhausted:
                st.error("‚ö†Ô∏è L√≠mite de solicitudes alcanzado. Por favor, espera un minuto.")
            except Exception as e:
                st.error(f"Ha ocurrido un error inesperado: {e}")
