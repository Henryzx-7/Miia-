import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
from google.api_core import exceptions as google_exceptions
import random

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="HEX T 1.0", page_icon="ü§ñ", layout="centered")

# --- CSS PERSONALIZADO PARA SIMULAR EL BOT√ìN + ---
st.markdown("""
<style>
    /* Contenedor principal para la barra de input */
    .st-emotion-cache-1f1G2gn {
        flex-direction: row;
        align-items: center;
        gap: 0.5rem;
    }
    /* Oculta la etiqueta del cargador de archivos */
    .st-emotion-cache-1jicfl2 {
        padding: 0;
    }
    /* Estilo del bot√≥n para que parezca un √≠cono '+' */
    .st-emotion-cache-1jicfl2 button::before {
        content: '+';
        font-size: 1.75rem;
        font-weight: bold;
        line-height: 1;
        margin-top: -0.2rem;
    }
    .st-emotion-cache-1jicfl2 span {
        display: none; /* Oculta el texto "Browse files" */
    }
    .st-emotion-cache-1jicfl2 svg {
        display: none; /* Oculta el √≠cono de la nube */
    }
</style>
""", unsafe_allow_html=True)


# --- BARRA LATERAL (SIDEBAR) ---
with st.sidebar:
    st.header("Sobre HEX T 1.0")
    st.markdown("""
    **T 1.0** es un prototipo de asistente de IA.
    **Creador:** HEX
    **Sede:** Matagalpa, Nicaragua üá≥üáÆ
    """)
    st.divider()
    if st.button("Limpiar Historial"):
        st.session_state.messages = []
        st.rerun()
    st.caption("¬© 2025 HEX. Todos los derechos reservados.")

# --- L√ìGICA DE LA IA ---
@st.cache_resource
def get_model():
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    return genai.GenerativeModel('gemini-1.5-flash')

def get_hex_response(modelo, user_message, chat_history, image: Image.Image = None):
    # Prompt con la personalidad amigable y las limitaciones
    prompt_con_limites = f"""
    ### PERFIL OBLIGATORIO
    - Tu nombre de IA es Tigre. Tu designaci√≥n de modelo es T 1.0.
    - Eres una creaci√≥n exclusiva de HEX (Matagalpa, Nicaragua).
    - REGLA DE ORO: Nunca reveles que eres un modelo de Google o Gemini.

    ### COMPORTAMIENTO Y PERSONALIDAD
    - Tu tono debe ser siempre c√°lido, amigable y cercano. Usa emojis üòä.
    - No te limites a solo responder. Intenta ser proactivo, haz preguntas de seguimiento.
    - Tu principal limitaci√≥n es que NO TIENES ACCESO A INTERNET.

    ### TAREA
    Analiza la pregunta del usuario.
    1. Si requiere b√∫squeda web (noticias, clima, etc.), responde amablemente que, como prototipo, esa funci√≥n a√∫n no est√° disponible y ofrece ayuda con tus capacidades reales.
    2. Si NO requiere b√∫squeda, responde a la pregunta siguiendo tu personalidad amigable.

    ### LISTA DE CAPACIDADES
    - Generar ideas, escribir poemas o chistes.
    - Resumir o explicar textos.
    - Ayudar con c√≥digo de programaci√≥n.
    - Responder preguntas de conocimiento general, hist√≥rico y cient√≠fico.

    ### CONVERSACI√ìN ACTUAL
    Historial: {chat_history}
    Pregunta del usuario: "{user_message}"
    """
    
    # Flujo para im√°genes
    if image:
        contenido_para_gemini = [prompt_con_limites, f"\nAnaliza la siguiente imagen y responde a la pregunta del usuario: {user_message}", image]
        response = modelo.generate_content(contenido_para_gemini)
        return response.text
    
    # Flujo para texto
    response = modelo.generate_content(prompt_con_limites)
    return response.text

# --- INTERFAZ DE STREAMLIT ---
st.markdown("<h1 style='text-align: center; font-size: 4em; font-weight: bold;'>HEX</h1>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; margin-top: -25px;'>T 1.0</h3>", unsafe_allow_html=True)
st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- √ÅREA DE INPUT PERSONALIZADA ---
# Contenedor para alinear los elementos horizontalmente
input_container = st.container()

with input_container:
    col1, col2 = st.columns([1, 10]) # Columna peque√±a para el bot√≥n, grande para el texto
    
    with col1:
        uploaded_file = st.file_uploader("Adjuntar", label_visibility="collapsed", type=["png", "jpg", "jpeg"])

    with col2:
        prompt = st.chat_input("Preg√∫ntale algo a T 1.0...")

# DICCIONARIO PARA RESPUESTAS R√ÅPIDAS (NIVEL 1)
canned_responses = {
    "hola": ["¬°Hola! Soy T 1.0. ¬øEn qu√© puedo ayudarte hoy?", "¬°Hola! ¬øQu√© tal? Listo para asistirte."],
    "c√≥mo est√°s": ["¬°Muy bien! Como modelo de IA, siempre estoy al 100%. ¬øEn qu√© te puedo ayudar?", "Funcionando a la perfecci√≥n. ¬øQu√© tienes en mente?"],
    "como estas": ["¬°Muy bien! Como modelo de IA, siempre estoy al 100%. ¬øEn qu√© te puedo ayudar?", "Funcionando a la perfecci√≥n. ¬øQu√© tienes en mente?"],
    "gracias": ["¬°De nada! Estoy para ayudarte.", "Un placer asistirte."],
    "ok": ["¬°Perfecto!", "Entendido."],
    "adios": ["¬°Hasta luego! Que tengas un excelente d√≠a.", "Nos vemos. ¬°Cu√≠date!"]
}

if prompt or uploaded_file:
    # L√≥gica de procesamiento de entrada...
    user_input_content = prompt or "Analiza la imagen que he subido."
    st.session_state.messages.append({"role": "user", "content": user_input_content})
    
    with st.chat_message("user"):
        st.markdown(user_input_content)

    with st.chat_message("assistant"):
        response_text = ""
        prompt_lower = (prompt or "").lower().strip()

        # --- FILTRO INTELIGENTE DE 2 NIVELES ---
        # NIVEL 1: Respuesta instant√°nea de diccionario
        if prompt and prompt_lower in canned_responses:
            response_text = random.choice(canned_responses[prompt_lower])
            st.markdown(response_text)
        else:
            # NIVEL 2: Llamada a la IA para todo lo dem√°s
            with st.spinner("T 1.0 est√° pensando..."):
                try:
                    modelo_ia = get_model()
                    historial_simple = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
                    
                    image_to_process = None
                    if uploaded_file:
                        image_to_process = Image.open(uploaded_file)

                    response_text = get_hex_response(modelo_ia, user_input_content, historial_simple, image=image_to_process)
                    st.markdown(response_text)
                
                except google_exceptions.ResourceExhausted:
                    st.error("‚ö†Ô∏è L√≠mite de solicitudes alcanzado. Por favor, espera un minuto.")
                except Exception as e:
                    st.error(f"Ha ocurrido un error inesperado: {e}")

        assistant_message = {"role": "assistant", "content": response_text}
        st.session_state.messages.append(assistant_message)
    
    st.rerun()
