import streamlit as st
import google.generativeai as genai
from duckduckgo_search import DDGS
import re
from PIL import Image
import io

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(
    page_title="HEX T 1.0",
    page_icon="ü§ñ",
    layout="centered",
    initial_sidebar_state="auto"
)

# --- BARRA LATERAL (SIDEBAR) ---
with st.sidebar:
    st.header("Sobre HEX T 1.0")
    st.markdown("""
    **T 1.0** es un prototipo de asistente de IA multimodal.
    
    **Creador:** HEX
    **Sede:** Matagalpa, Nicaragua üá≥üáÆ
    
    Puedes chatear con texto o subir una imagen para que la analice.
    """)
    st.divider()
    st.caption("¬© 2025 HEX. Todos los derechos reservados.")


# --- L√ìGICA DE LA IA ---
@st.cache_resource
def get_model():
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    return genai.GenerativeModel('gemini-1.5-flash')

def search_duckduckgo(query: str):
    try:
        with DDGS() as ddgs:
            results = [r['body'] for r in ddgs.text(query, max_results=3)]
            return "\n".join(results) if results else "No se encontraron resultados."
    except Exception:
        return "Error al buscar en la web."

# Versi√≥n final y m√°s robusta de la funci√≥n de respuesta
def get_hex_response(modelo, user_message, chat_history, image: Image.Image = None):
    # Flujo para im√°genes (no cambia)
    if image:
        prompt_final = f"""
        Eres "T 1.0" de HEX. Analiza la imagen y responde a la pregunta del usuario de forma amigable y detallada.
        Pregunta: "{user_message}"
        """
        contenido_para_gemini = [prompt_final, image]
        response = modelo.generate_content(contenido_para_gemini)
        return response.text
    
    # --- NUEVA L√ìGICA PARA TEXTO: BUSCAR SIEMPRE ---
    
    # 1. El c√≥digo ahora siempre busca en la web primero.
    print(f"ü§ñ Buscando en la web sobre: '{user_message}'")
    informacion_buscada = search_duckduckgo(user_message)
    
    # 2. Se construye un √∫nico prompt que incluye el contexto de la b√∫squeda.
    prompt_final = f"""
    # IDENTIDAD Y PERSONALIDAD
    Eres "T 1.0" de HEX, un asistente amigable y conversacional. La "T" es por Tigre. Tu creador es HEX (Matagalpa, Nicaragua). NUNCA menciones que usas tecnolog√≠a de Google o Gemini. Cuando escribas c√≥digo, usa el formato ```python\n...c√≥digo...\n```.

    # TAREA
    Responde a la pregunta del usuario. Para ayudarte, he realizado una b√∫squeda en la web.

    # CONTEXTO DE LA B√öSQUEDA WEB
    ---
    {informacion_buscada}
    ---

    # INSTRUCCIONES
    Usa el contexto de la b√∫squeda si es relevante y √∫til para responder la pregunta. Si el contexto no ayuda, es irrelevante, o no tiene nada que ver, ign√≥ralo por completo y responde con tu propio conocimiento.

    # CONVERSACI√ìN
    Historial: {chat_history}
    Pregunta del usuario: "{user_message}"
    """
    
    # 3. Se genera la respuesta en un solo paso.
    response = modelo.generate_content(prompt_final)
    return response.text

# --- INTERFAZ DE STREAMLIT ---
st.title("ü§ñ HEX T 1.0")
st.caption("Un asistente de lenguaje avanzado creado por HEX.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if "image" in message:
            st.image(message["image"], width=200)
        st.markdown(message["content"])

# √Årea para subir archivos
uploaded_file = st.file_uploader("¬øQuieres analizar una imagen?", type=["png", "jpg", "jpeg"])

# Input de texto
prompt = st.chat_input("Preg√∫ntale algo al modelo T 1.0...")

if prompt or uploaded_file:
    user_input = {"role": "user", "content": prompt or "Analiza esta imagen."}
    image_to_process = None

    if uploaded_file:
        image = Image.open(uploaded_file)
        buf = io.BytesIO()
        image.save(buf, format="PNG")
        image_bytes = buf.getvalue()
        user_input["image"] = image_bytes
        image_to_process = image
    
    st.session_state.messages.append(user_input)
    with st.chat_message("user"):
        if uploaded_file:
            st.image(image_to_process, width=200)
        st.markdown(prompt or "Analiza esta imagen.")

    with st.chat_message("assistant"):
        with st.spinner("T 1.0 est√° pensando..."):
            modelo_ia = get_model()
            historial_simple = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
            response = get_hex_response(modelo_ia, prompt or "Describe la imagen.", historial_simple, image=image_to_process)
            st.markdown(response, unsafe_allow_html=True)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
